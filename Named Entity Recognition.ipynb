{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Misran Dolan\n",
    "## Named Entity Recognition Research"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This research was used for a client in the medical industry that needed a tool to extract information on insurance claims. The information could be treatment, date, symptom, etc. Named Entity Recognition is a natural language processing technique used by many Python libraries to categorize text based on pre-defined indicators.\n",
    "\n",
    "### The first NER Demo uses the library spaCy, and the second library uses the Stanford NER Tagger"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## spaCy demo\n",
    "\n",
    "### Faster, puts entire entity into one cell\n",
    "\n",
    "#### Doesn't detect \"st\" suffixes\n",
    "\n",
    "#### Is able to detect institutions (ICU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: John Smith was admitted for a routine surgery on Thursday May 23rd 2017.  He had complications overnight and was transferred to the ICU.\n",
      "Type: PERSON, Value: John Smith\n",
      "Type: DATE, Value: Thursday May 23rd 2017\n",
      "Type: ORG, Value: ICU\n"
     ]
    }
   ],
   "source": [
    "import spacy\n",
    "\n",
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "article = 'John Smith was admitted for a routine surgery on Thursday May 23rd 2017.  He had complications overnight and was transferred to the ICU.'\n",
    "\n",
    "document = spacy_nlp(article)\n",
    "\n",
    "print('Original Sentence: %s' % (article))\n",
    "\n",
    "for element in document.ents:\n",
    "    print('Type: %s, Value: %s' % (element.label_, element))\n",
    "    \n",
    "#can do 'rd', but not 'st'\n",
    "#didn't detect 'overnight'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(John Smith, Thursday May 23rd 2017, ICU)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document.ents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stanford NER Demo\n",
    "\n",
    "### Has to be used through NLTK API\n",
    "### Slower than spaCy, but it splits each entity into each cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NTLK Version: 3.4\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "from nltk.tag import StanfordNERTagger\n",
    "import os\n",
    "#java_path = \"C:/Program Files (x86)/Java/jre1.8.0_211/bin/java.exe\"\n",
    "#os.environ['JAVAHOME'] = java_path\n",
    "\n",
    "print('NTLK Version: %s' % nltk.__version__)\n",
    "\n",
    "stanford_ner_tagger = StanfordNERTagger(\n",
    "    '/Users/ladmin/Documents/stanford_ner/classifiers/english.muc.7class.distsim.crf.ser.gz',\n",
    "    '/Users/ladmin/Documents/stanford_ner/stanford-ner.jar'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/ladmin/nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Sentence: John Smith was admitted for a routine surgery on Thursday May 21st 2017.  He had complications overnight and was transferred to the ICU.\n",
      "Type: PERSON, Value: John\n",
      "Type: PERSON, Value: Smith\n",
      "Type: DATE, Value: Thursday\n",
      "Type: DATE, Value: May\n",
      "Type: DATE, Value: 21st\n",
      "Type: DATE, Value: 2017.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "article = 'John Smith was admitted for a routine surgery on Thursday May 21st 2017.  He had complications overnight and was transferred to the ICU.'\n",
    "\n",
    "results = stanford_ner_tagger.tag(article.split())\n",
    "\n",
    "print('Original Sentence: %s' % (article))\n",
    "for result in results:\n",
    "    tag_value = result[0]\n",
    "    tag_type = result[1]\n",
    "    if tag_type != 'O':\n",
    "        print('Type: %s, Value: %s' % (tag_type, tag_value))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Demos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/ladmin/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     /Users/ladmin/nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to /Users/ladmin/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "John Smith was admitted for a routine surgery on Thursday May 21st 2017.  He had complications overnight and was transferred to the ICU.\n",
      "PERSON John\n",
      "PERSON Smith\n",
      "ORGANIZATION ICU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping corpora/words.zip.\n"
     ]
    }
   ],
   "source": [
    "nltk.download('averaged_perceptron_tagger')\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download('words')\n",
    "print(article)\n",
    "for sent in nltk.sent_tokenize(article):\n",
    "    for chunk in nltk.ne_chunk(nltk.pos_tag(nltk.word_tokenize(sent))):\n",
    "        if hasattr(chunk, 'label'):\n",
    "             print(chunk.label(), ' '.join(c[0] for c in chunk))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: DATE, Value: Tuesday\n",
      "Type: LOCATION, Value: Europe\n",
      "Type: ORGANIZATION, Value: Asia-Pacific\n",
      "Type: LOCATION, Value: Japan\n",
      "Type: PERCENT, Value: 1.7\n",
      "Type: PERCENT, Value: percent\n",
      "Type: ORGANIZATION, Value: Nikkei\n",
      "Type: PERCENT, Value: 3.1\n",
      "Type: PERCENT, Value: percent\n",
      "Type: LOCATION, Value: European\n",
      "Type: LOCATION, Value: Union\n",
      "Type: PERSON, Value: Theresa\n",
      "Type: PERSON, Value: May\n"
     ]
    }
   ],
   "source": [
    "article = '''\n",
    "Asian shares skidded on Tuesday after a rout in tech stocks put Wall Street to the sword, while a \n",
    "sharp drop in oil prices and political risks in Europe pushed the dollar to 16-month highs as investors dumped \n",
    "riskier assets. MSCI’s broadest index of Asia-Pacific shares outside Japan dropped 1.7 percent to a 1-1/2 \n",
    "week trough, with Australian shares sinking 1.6 percent. Japan’s Nikkei dived 3.1 percent led by losses in \n",
    "electric machinery makers and suppliers of Apple’s iphone parts. Sterling fell to $1.286 after three straight \n",
    "sessions of losses took it to the lowest since Nov.1 as there were still considerable unresolved issues with the\n",
    "European Union over Brexit, British Prime Minister Theresa May said on Monday.'''\n",
    "\n",
    "results = stanford_ner_tagger.tag(article.split())\n",
    "\n",
    "#print('Original Sentence: %s' % (article))\n",
    "for result in results:\n",
    "    tag_value = result[0]\n",
    "    tag_type = result[1]\n",
    "    if tag_type != 'O':\n",
    "        print('Type: %s, Value: %s' % (tag_type, tag_value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Type: NORP, Value: Asian\n",
      "Type: DATE, Value: Tuesday\n",
      "Type: LOC, Value: Europe\n",
      "Type: ORG, Value: MSCI\n",
      "Type: LOC, Value: Asia-Pacific\n",
      "Type: GPE, Value: Japan\n",
      "Type: PERCENT, Value: 1.7 percent\n",
      "Type: DATE, Value: week\n",
      "Type: NORP, Value: Australian\n",
      "Type: PERCENT, Value: 1.6 percent\n",
      "Type: GPE, Value: Japan\n",
      "Type: PERCENT, Value: 3.1 percent\n",
      "Type: ORG, Value: Apple\n",
      "Type: MONEY, Value: 1.286\n",
      "Type: CARDINAL, Value: three\n",
      "Type: GPE, Value: Nov.1\n",
      "Type: ORG, Value: European Union\n",
      "Type: GPE, Value: Brexit\n",
      "Type: NORP, Value: British\n",
      "Type: PERSON, Value: Theresa\n",
      "Type: DATE, Value: May\n",
      "Type: DATE, Value: Monday\n"
     ]
    }
   ],
   "source": [
    "spacy_nlp = spacy.load('en_core_web_sm')\n",
    "document = spacy_nlp(article)\n",
    "\n",
    "#print('Original Sentence: %s' % (article))\n",
    "\n",
    "for element in document.ents:\n",
    "    print('Type: %s, Value: %s' % (element.label_, element))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
